"""
DocIntel Lab - Streamlit Application.
Integriert: Analyse (mit Native/Scanned), Ground Truth Editor, Benchmark.
"""

import streamlit as st
import pandas as pd
import json

from core import DocumentOrchestrator, get_available_extractors
from benchmark import GroundTruthManifest, DocumentGroundTruth, BenchmarkRunner

# === Konfiguration ===
st.set_page_config(
    page_title="DocIntel Lab",
    layout="wide",
    page_icon="ðŸ”¬",
    initial_sidebar_state="expanded"
)

st.markdown("""
<style>
    [data-testid="stMetricValue"] { font-size: 1.1rem; }
</style>
""", unsafe_allow_html=True)

# === Session State ===
if "manifest" not in st.session_state:
    st.session_state.manifest = GroundTruthManifest()
if "benchmark_result" not in st.session_state:
    st.session_state.benchmark_result = None


# === Cached Resources ===
@st.cache_resource
def get_orchestrator():
    return DocumentOrchestrator()


# === Sidebar Navigation ===
st.sidebar.title("ðŸ”¬ DocIntel Lab")

page = st.sidebar.radio(
    "Navigation",
    ["ðŸ“„ Analyse", "ðŸŽ¯ Ground Truth", "ðŸ“Š Benchmark"],
    label_visibility="collapsed"
)

st.sidebar.markdown("---")
st.sidebar.caption(f"**Ground Truth:** {len(st.session_state.manifest.documents)} EintrÃ¤ge")


# ============================================================
# SEITE 1: ANALYSE (Original-FunktionalitÃ¤t erhalten)
# ============================================================
if page == "ðŸ“„ Analyse":
    st.title("ðŸ”¬ Scientific Document Intelligence")
    st.markdown("""
    **Pipeline:** `Identifier` (Magika) â†’ `Inspector` (Native/Scanned) â†’ `Extractor` (Tabellen/Bilder)
    """)
    
    orch = get_orchestrator()
    
    uploaded_files = st.file_uploader(
        "PDF Dateien hochladen",
        type=["pdf"],
        accept_multiple_files=True
    )
    
    if uploaded_files:
        all_results = []
        
        st.divider()
        st.subheader(f"ðŸ“‘ Analyse-Ergebnisse ({len(uploaded_files)} Dateien)")
        
        for uploaded_file in uploaded_files:
            file_bytes = uploaded_file.getvalue()
            result = orch.run_pipeline(file_bytes, uploaded_file.name)
            all_results.append(result)
            
            pdf_details = result.get("pdf_details") or {}
            layout_stats = result.get("layout_stats") or {}
            
            # UI pro Datei
            with st.expander(f"ðŸ“„ **{result['filename']}** ({result['format'].upper()})", expanded=True):
                
                # Header: Typ & Reasoning
                c1, c2 = st.columns([3, 1])
                with c1:
                    sub_type = pdf_details.get("sub_type", "N/A")
                    colors = {"NATIVE": "green", "SCANNED": "orange", "VECTOR_GRAPHIC": "blue"}
                    badge_color = colors.get(sub_type, "gray")
                    
                    st.markdown(f"**Typ:** :{badge_color}[{sub_type}] | **MIME:** `{result['mime']}`")
                    st.info(f"ðŸ’¡ **Reasoning:** {result['reasoning']}")
                
                with c2:
                    st.metric("Konfidenz", f"{result['confidence']:.1%}")
                
                # Detail-Metriken
                if layout_stats:
                    st.markdown("---")
                    m1, m2, m3, m4 = st.columns(4)
                    
                    m1.metric("Seiten", pdf_details.get("pages", 0))
                    text_cov = pdf_details.get("text_coverage_pct", 0)
                    m1.caption(f"ðŸ“ {text_cov:.0f} chars/page")
                    
                    # Tabellen mit GT-Vergleich
                    gt = st.session_state.manifest.get(uploaded_file.name)
                    tables = layout_stats.get("tables", 0)
                    if gt:
                        diff = tables - gt.table_count
                        if diff == 0:
                            m2.metric("Tabellen", tables, delta="âœ“ GT", delta_color="off")
                        else:
                            m2.metric("Tabellen", tables, delta=f"{diff:+d} vs GT", 
                                     delta_color="inverse" if diff > 0 else "normal")
                    else:
                        m2.metric("Tabellen", tables)
                    
                    # Bilder
                    u_imgs = layout_stats.get("images", 0)
                    t_imgs = layout_stats.get("images_total", 0)
                    m3.metric("Bilder (Unique)", u_imgs, delta=f"Total: {t_imgs}", delta_color="off")
                    
                    # Paragraphen & Mathe
                    paras = layout_stats.get("paragraphs", 0)
                    math = layout_stats.get("math_formulas", 0)
                    m4.metric("Text-BlÃ¶cke", paras)
                    m4.caption(f"ðŸ§® Mathe: {'Hoch' if math > 0 else 'Niedrig'}")
                    
                    # OCR Hinweis
                    if layout_stats.get("requires_ocr"):
                        st.warning("âš ï¸ Gescanntes PDF - fÃ¼r Tabellen-/Texterkennung ist OCR erforderlich.")
                
                # Quick-Add zu Ground Truth
                col1, col2 = st.columns([3, 1])
                with col2:
                    if not gt:
                        if st.button("âž• Zu GT hinzufÃ¼gen", key=f"add_{uploaded_file.name}"):
                            st.session_state.manifest.add(DocumentGroundTruth(
                                file_name=uploaded_file.name,
                                table_count=layout_stats.get("tables", 0),
                                image_count=layout_stats.get("images", 0),
                                pages=layout_stats.get("pages", 0)
                            ))
                            st.rerun()
                    else:
                        st.caption(f"âœ“ In GT (T={gt.table_count}, I={gt.image_count})")
                
                # Raw JSON
                with st.expander("ðŸ› ï¸ Rohe JSON-Daten"):
                    st.json(result)
        
        # Export
        if all_results:
            st.divider()
            st.subheader("ðŸ“Š Export")
            
            flat_data = []
            for r in all_results:
                pdf = r.get("pdf_details") or {}
                lay = r.get("layout_stats") or {}
                flat_data.append({
                    "Filename": r["filename"],
                    "Format": r["format"],
                    "Sub-Type": pdf.get("sub_type", "N/A"),
                    "Confidence": r["confidence"],
                    "Pages": pdf.get("pages", 0),
                    "Tables": lay.get("tables", 0),
                    "Images_Unique": lay.get("images", 0),
                    "Images_Total": lay.get("images_total", 0),
                    "Paragraphs": lay.get("paragraphs", 0),
                    "Math": lay.get("math_formulas", 0),
                })
            
            df = pd.DataFrame(flat_data)
            st.dataframe(df, use_container_width=True)
            
            st.download_button(
                "ðŸ“¥ Als CSV",
                df.to_csv(index=False).encode('utf-8'),
                "analyse_export.csv",
                "text/csv"
            )


# ============================================================
# SEITE 2: GROUND TRUTH EDITOR
# ============================================================
elif page == "ðŸŽ¯ Ground Truth":
    st.title("ðŸŽ¯ Ground Truth Editor")
    st.markdown("""
    Definiere die **korrekten** Werte fÃ¼r deine Test-PDFs.
    
    **So geht's:**
    1. Lade PDFs in der Analyse-Seite hoch
    2. Ã–ffne jedes PDF manuell und zÃ¤hle die echten Tabellen/Bilder
    3. Trage hier die korrekten Werte ein
    4. Speichere das Manifest (JSON) fÃ¼r spÃ¤tere Nutzung
    """)
    
    # Import/Export
    col1, col2, col3 = st.columns(3)
    
    with col1:
        uploaded_manifest = st.file_uploader("ðŸ“‚ Manifest laden", type=["json"], key="load_manifest")
        if uploaded_manifest:
            try:
                data = json.loads(uploaded_manifest.getvalue().decode('utf-8'))
                st.session_state.manifest = GroundTruthManifest(
                    documents=[DocumentGroundTruth.from_dict(d) for d in data.get("documents", [])]
                )
                st.success(f"âœ“ {len(st.session_state.manifest.documents)} EintrÃ¤ge geladen")
                st.rerun()
            except Exception as e:
                st.error(f"Fehler: {e}")
    
    with col2:
        if st.session_state.manifest.documents:
            manifest_json = json.dumps(
                {"documents": [d.to_dict() for d in st.session_state.manifest.documents]},
                indent=2, ensure_ascii=False
            )
            st.download_button(
                "ðŸ’¾ Manifest speichern",
                manifest_json.encode('utf-8'),
                "ground_truth.json",
                "application/json"
            )
    
    with col3:
        if st.button("ðŸ—‘ï¸ Alle lÃ¶schen"):
            st.session_state.manifest = GroundTruthManifest()
            st.rerun()
    
    st.markdown("---")
    
    # Neuer Eintrag
    st.subheader("âž• Neuen Eintrag hinzufÃ¼gen")
    
    with st.form("add_gt"):
        c1, c2 = st.columns(2)
        
        with c1:
            new_file = st.text_input("PDF Dateiname", placeholder="dokument.pdf")
            new_tables = st.number_input("Korrekte Tabellen-Anzahl", min_value=0, value=0)
            new_images = st.number_input("Korrekte Bilder-Anzahl (Unique)", min_value=0, value=0)
        
        with c2:
            new_pages = st.number_input("Seiten", min_value=0, value=0)
            new_category = st.selectbox(
                "Kategorie",
                ["general", "simple_table", "multi_table", "spanning_table", "borderless", "scanned"]
            )
            new_difficulty = st.slider("Schwierigkeit", 1, 5, 1)
        
        new_notes = st.text_input("Notizen")
        
        if st.form_submit_button("HinzufÃ¼gen", type="primary"):
            if new_file:
                if not new_file.endswith('.pdf'):
                    new_file += '.pdf'
                st.session_state.manifest.add(DocumentGroundTruth(
                    file_name=new_file,
                    table_count=new_tables,
                    image_count=new_images,
                    pages=new_pages,
                    category=new_category,
                    difficulty=new_difficulty,
                    notes=new_notes
                ))
                st.success(f"âœ“ '{new_file}' hinzugefÃ¼gt")
                st.rerun()
            else:
                st.error("Dateiname erforderlich")
    
    # Bestehende EintrÃ¤ge
    if st.session_state.manifest.documents:
        st.markdown("---")
        st.subheader(f"ðŸ“‹ EintrÃ¤ge ({len(st.session_state.manifest.documents)})")
        
        for i, doc in enumerate(st.session_state.manifest.documents):
            with st.expander(f"ðŸ“„ {doc.file_name} | T={doc.table_count}, I={doc.image_count}"):
                c1, c2, c3 = st.columns([2, 2, 1])
                
                with c1:
                    new_t = st.number_input("Tabellen", value=doc.table_count, key=f"t_{i}")
                    new_i = st.number_input("Bilder", value=doc.image_count, key=f"i_{i}")
                
                with c2:
                    st.text(f"Kategorie: {doc.category}")
                    st.text(f"Schwierigkeit: {'â­' * doc.difficulty}")
                    if doc.notes:
                        st.caption(f"ðŸ“ {doc.notes}")
                
                with c3:
                    if st.button("ðŸ’¾", key=f"save_{i}", help="Speichern"):
                        doc.table_count = new_t
                        doc.image_count = new_i
                        st.rerun()
                    if st.button("ðŸ—‘ï¸", key=f"del_{i}", help="LÃ¶schen"):
                        st.session_state.manifest.remove(doc.file_name)
                        st.rerun()


# ============================================================
# SEITE 3: BENCHMARK
# ============================================================
elif page == "ðŸ“Š Benchmark":
    st.title("ðŸ“Š Benchmark")
    st.markdown("Vergleiche verschiedene Extraktions-Tools gegen deine Ground Truth.")
    
    # Status Check
    if not st.session_state.manifest.documents:
        st.warning("âš ï¸ Keine Ground Truth definiert. Gehe zum Ground Truth Editor.")
        st.stop()
    
    # VerfÃ¼gbare Tools
    extractors = get_available_extractors()
    st.info(f"**VerfÃ¼gbare Tools:** {', '.join(e.name for e in extractors)}")
    
    st.markdown("---")
    
    # File Upload
    st.subheader("ðŸ“ PDFs fÃ¼r Benchmark hochladen")
    st.caption("Dateinamen mÃ¼ssen mit Ground Truth EintrÃ¤gen Ã¼bereinstimmen")
    
    benchmark_files = st.file_uploader(
        "PDFs auswÃ¤hlen",
        type=["pdf"],
        accept_multiple_files=True,
        key="benchmark_files"
    )
    
    if benchmark_files:
        # Matching anzeigen
        matched = []
        unmatched = []
        
        for f in benchmark_files:
            gt = st.session_state.manifest.get(f.name)
            if gt:
                matched.append((f.name, gt))
            else:
                unmatched.append(f.name)
        
        c1, c2 = st.columns(2)
        with c1:
            if matched:
                st.success(f"âœ“ {len(matched)} mit Ground Truth")
                for name, gt in matched:
                    st.caption(f"  â€¢ {name} (T={gt.table_count}, I={gt.image_count})")
        with c2:
            if unmatched:
                st.warning(f"âš ï¸ {len(unmatched)} ohne Ground Truth")
                for name in unmatched:
                    st.caption(f"  â€¢ {name}")
        
        # Start Benchmark
        if matched and st.button("ðŸš€ Benchmark starten", type="primary"):
            runner = BenchmarkRunner(
                manifest=st.session_state.manifest,
                extractors=extractors
            )
            
            files_data = [(f.name, f.getvalue()) for f in benchmark_files]
            
            with st.spinner("Benchmark lÃ¤uft..."):
                result = runner.run(files_data)
            
            st.session_state.benchmark_result = result
            st.success("âœ“ Fertig!")
    
    # Ergebnisse
    if st.session_state.benchmark_result:
        result = st.session_state.benchmark_result
        
        st.markdown("---")
        st.subheader("ðŸ“ˆ Ergebnisse")
        
        # Summary
        df_summary = pd.DataFrame(result.to_summary_list())
        st.dataframe(df_summary, use_container_width=True, hide_index=True)
        
        # Rankings
        st.subheader("ðŸ† Rankings")
        c1, c2, c3 = st.columns(3)
        
        with c1:
            st.markdown("**Tabellen-Genauigkeit:**")
            for i, (name, acc) in enumerate(result.get_ranking("table_accuracy")[:3], 1):
                medal = ["ðŸ¥‡", "ðŸ¥ˆ", "ðŸ¥‰"][i-1]
                st.write(f"{medal} {name}: {acc:.0%}")
        
        with c2:
            st.markdown("**Bild-Genauigkeit:**")
            for i, (name, acc) in enumerate(result.get_ranking("image_accuracy")[:3], 1):
                medal = ["ðŸ¥‡", "ðŸ¥ˆ", "ðŸ¥‰"][i-1]
                st.write(f"{medal} {name}: {acc:.0%}")
        
        with c3:
            st.markdown("**Geschwindigkeit:**")
            for i, (name, ms) in enumerate(result.get_ranking("avg_time_ms")[:3], 1):
                medal = ["ðŸ¥‡", "ðŸ¥ˆ", "ðŸ¥‰"][i-1]
                st.write(f"{medal} {name}: {ms:.0f}ms")
        
        # Details
        if result.detailed_results:
            st.markdown("---")
            st.subheader("ðŸ“‹ Details")
            
            df_details = pd.DataFrame(result.detailed_results)
            
            # FÃ¤rbung fÃ¼r Diff-Spalten
            def highlight_diff(val):
                if val is None:
                    return ""
                if val == 0:
                    return "background-color: #d4edda"
                elif val > 0:
                    return "background-color: #fff3cd"
                else:
                    return "background-color: #f8d7da"
            
            styled = df_details.style.applymap(highlight_diff, subset=["table_diff", "image_diff"])
            st.dataframe(styled, use_container_width=True, hide_index=True)
            
            st.download_button(
                "ðŸ“¥ Details als CSV",
                df_details.to_csv(index=False).encode('utf-8'),
                "benchmark_details.csv",
                "text/csv"
            )
